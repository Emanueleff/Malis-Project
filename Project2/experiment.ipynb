{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron as PerceptronSci\n",
    "import numpy as np\n",
    "from perceptron import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 64)\n",
      "(72, 64)\n",
      "(72, 64)\n"
     ]
    }
   ],
   "source": [
    "[X, y] = load_digits(n_class=2, return_X_y=True)\n",
    "\n",
    "#print(X.shape)\n",
    "#print(y.shape)\n",
    "y[y==0] = -1\n",
    "\n",
    "#Split in training and test in 60-20-20\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, train_size=0.6, random_state=10)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, train_size=0.5, random_state=10)\n",
    "\n",
    "#check the result obtained\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0\n",
      "The results are equal: True\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.0001\n",
    "\n",
    "# Training our model\n",
    "model = Perceptron(X_train, y_train)\n",
    "model.train(alpha)\n",
    "\n",
    "# Prediction with our model\n",
    "y_hat = model.predict(X_val)\n",
    "#print(y_hat)\n",
    "\n",
    "# Accuracy esteem\n",
    "acc = np.sum(y_hat == y_val)/y_val.size * 100\n",
    "print(f'Accuracy: {acc}')\n",
    "\n",
    "# Training and prediction with scikit perceptron model\n",
    "example = PerceptronSci(alpha=alpha)\n",
    "example.fit(X_train,y_train)\n",
    "y_ex = example.predict(X_val)\n",
    "\n",
    "# Check the results are the same\n",
    "equal = y_ex == y_hat\n",
    "print(f'The results are equal: {not equal[equal==False].size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model with different alphas to see if there is any change in the speed of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 training time: 0.0020215511322021484\n",
      "0.0001 training time: 0.0003275871276855469\n",
      "0.001 training time: 3.0517578125e-05\n",
      "0.01 training time: 2.2411346435546875e-05\n",
      "0.1 training time: 2.193450927734375e-05\n",
      "1.0 training time: 2.2172927856445312e-05\n",
      "10.0 training time: 2.0742416381835938e-05\n",
      "100.0 training time: 2.1457672119140625e-05\n",
      "1000.0 training time: 2.0742416381835938e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model = Perceptron(X_train, y_train)\n",
    "\n",
    "alpha = 0.00001\n",
    "while (alpha<2000): \n",
    "        start_time = time.time()\n",
    "        model.train(alpha)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"{alpha} training time: {execution_time}\")\n",
    "        # print(model.weights)        \n",
    "        alpha = alpha * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing heavely remarkable has been noted, but alpha too low takes a lot more time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
